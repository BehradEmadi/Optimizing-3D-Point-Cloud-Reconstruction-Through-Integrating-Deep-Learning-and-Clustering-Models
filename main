#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Title: Variational Autoencoder and Clustering Analysis

Description:
    This script loads a dataset, preprocesses the data, trains a Variational Autoencoder (VAE),
    and applies various clustering algorithms on the encoded features. It then evaluates
    the clustering performance using silhouette, Calinski-Harabasz, and Davies-Bouldin metrics,
    and visualizes the clustering results.

Usage:
    python main.py --data_path path/to/your/datafile.txt

Author: BehRaD
Created on: 2024-12-12
"""

import os
import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import mse
from sklearn.preprocessing import StandardScaler
from sklearn.mixture import GaussianMixture
from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering
from sklearn.neighbors import kneighbors_graph
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

# Suppress TensorFlow GPU warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


def load_and_preprocess_data(filepath: str) -> (pd.DataFrame, np.ndarray):
    """
    Loads data from a CSV file, assigns column names, converts specific columns to numeric,
    drops rows with missing values, and scales the data.
    
    Args:
        filepath (str): Path to the data file.
        
    Returns:
        X (pd.DataFrame): Original DataFrame with the clustering labels added later.
        X_scaled (np.ndarray): Scaled numerical features for training.
    """
    # Load the dataset; adjust header and skiprows as necessary
    X = pd.read_csv(filepath, header=None, skiprows=1, dtype=str)
    X.columns = ['indice', 'X', 'Y', 'Z', 'error_proj', 'ac', 'cant_fotos', 'dist', 'med_ang']
    
    # Convert specified columns to numeric and drop rows with missing values
    numeric_cols = ['error_proj', 'ac', 'cant_fotos', 'dist', 'med_ang']
    X[numeric_cols] = X[numeric_cols].apply(pd.to_numeric, errors='coerce')
    X = X.dropna(subset=numeric_cols)
    
    # Standardize the numeric features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X[numeric_cols])
    
    return X, X_scaled


def sampling(args):
    """
    Reparameterization trick by sampling from an isotropic unit Gaussian.
    
    Args:
        args (tuple): Mean and log of variance of Q(z|X)
        
    Returns:
        z (Tensor): Sampled latent vector
    """
    z_mean, z_log_var = args
    batch = tf.shape(z_mean)[0]
    latent_dim = tf.shape(z_mean)[1]
    epsilon = tf.keras.backend.random_normal(shape=(batch, latent_dim))
    return z_mean + tf.keras.backend.exp(0.5 * z_log_var) * epsilon


def build_vae(input_dim: int, latent_dim: int = 2, intermediate_dim: int = 128) -> (Model, Model):
    """
    Constructs a Variational Autoencoder (VAE) with an encoder and decoder.
    
    Args:
        input_dim (int): Dimensionality of the input.
        latent_dim (int): Size of the latent space (default is 2 for 2D latent space).
        intermediate_dim (int): Number of nodes in the intermediate layers.
        
    Returns:
        vae (Model): The complete VAE model.
        encoder (Model): The encoder part of the VAE for generating latent representations.
    """
    # Encoder
    inputs = Input(shape=(input_dim,), name='encoder_input')
    x = Dense(intermediate_dim, activation='relu',
              kernel_regularizer=tf.keras.regularizers.l2(0.01))(inputs)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)
    x = Dense(intermediate_dim // 2, activation='relu',
              kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)
    
    # Latent space parameters
    z_mean = Dense(latent_dim, name='z_mean')(x)
    z_log_var = Dense(latent_dim, name='z_log_var')(x)
    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])
    
    # Decoder
    decoder_input = Input(shape=(latent_dim,), name='decoder_input')
    decoder_x = Dense(intermediate_dim // 2, activation='relu')(decoder_input)
    decoder_x = BatchNormalization()(decoder_x)
    decoder_x = Dropout(0.2)(decoder_x)
    decoder_x = Dense(intermediate_dim, activation='relu')(decoder_x)
    decoder_output = Dense(input_dim, activation='linear', name='decoder_output')(decoder_x)
    decoder = Model(decoder_input, decoder_output, name='decoder')
    
    # VAE model
    outputs = decoder(z)
    vae = Model(inputs, outputs, name='vae')
    
    # Loss function: Reconstruction loss + KL divergence loss
    reconstruction_loss = mse(inputs, outputs) * input_dim
    kl_loss = -0.5 * tf.keras.backend.sum(
        1 + z_log_var - tf.keras.backend.square(z_mean) - tf.keras.backend.exp(z_log_var), axis=-1
    )
    vae_loss = tf.keras.backend.mean(reconstruction_loss + kl_loss)
    vae.add_loss(vae_loss)
    vae.compile(optimizer=Adam(learning_rate=0.001))
    
    # Encoder model for latent space extraction
    encoder = Model(inputs, z_mean, name='encoder')
    
    return vae, encoder


def train_vae(vae: Model, X_scaled: np.ndarray, epochs: int = 100, batch_size: int = 256):
    """
    Trains the VAE model on the provided scaled data.
    
    Args:
        vae (Model): The Variational Autoencoder model.
        X_scaled (np.ndarray): The standardized input data.
        epochs (int): Number of training epochs.
        batch_size (int): Batch size for training.
    """
    vae.fit(X_scaled, X_scaled, epochs=epochs, batch_size=batch_size,
            validation_split=0.2, shuffle=True)


def perform_clustering(X_encoded: np.ndarray, X_df: pd.DataFrame, n_clusters: int = 10) -> pd.DataFrame:
    """
    Applies various clustering algorithms on the encoded data and stores the cluster labels in the DataFrame.
    
    Args:
        X_encoded (np.ndarray): Encoded data from the VAE.
        X_df (pd.DataFrame): Original DataFrame to which clustering labels will be added.
        n_clusters (int): Number of clusters to form (default is 10).
        
    Returns:
        X_df (pd.DataFrame): DataFrame with added cluster labels.
    """
    # Gaussian Mixture Model (GMM)
    gmm = GaussianMixture(n_components=n_clusters, n_init=10, tol=1e-4, covariance_type='full')
    X_df['cluster_gmm'] = gmm.fit_predict(X_encoded)
    
    # K-Means Clustering
    kmeans = KMeans(n_clusters=n_clusters, n_init=50, tol=1e-5, random_state=42)
    X_df['cluster_kmeans'] = kmeans.fit_predict(X_encoded)
    
    # Agglomerative Clustering with connectivity constraints
    connectivity = kneighbors_graph(X_encoded, n_neighbors=20, include_self=False)
    agglo = AgglomerativeClustering(n_clusters=n_clusters, connectivity=connectivity)
    X_df['cluster_agglo'] = agglo.fit_predict(X_encoded)
    
    # Spectral Clustering
    spectral = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', random_state=42)
    X_df['cluster_spectral'] = spectral.fit_predict(X_encoded)
    
    return X_df


def evaluate_clusters(X_encoded: np.ndarray, cluster_labels: dict):
    """
    Computes evaluation metrics for each clustering algorithm and prints the results.
    
    Args:
        X_encoded (np.ndarray): Encoded data from the VAE.
        cluster_labels (dict): A dictionary mapping clustering method names to their labels.
    """
    evaluation_results = {}
    
    for method, labels in cluster_labels.items():
        silhouette = silhouette_score(X_encoded, labels)
        calinski = calinski_harabasz_score(X_encoded, labels)
        davies = davies_bouldin_score(X_encoded, labels)
        evaluation_results[method] = {
            'Silhouette': silhouette,
            'Calinski-Harabasz': calinski,
            'Davies-Bouldin': davies
        }
    
    # Print evaluation metrics
    for method, scores in evaluation_results.items():
        print(f"\n{method} Clustering Evaluation Metrics:")
        for metric, score in scores.items():
            print(f"{metric}: {score:.4f}")


def plot_clusters(X_encoded: np.ndarray, cluster_labels: dict):
    """
    Visualizes clustering results in a 2D latent space.
    
    Args:
        X_encoded (np.ndarray): Encoded 2D data from the VAE.
        cluster_labels (dict): A dictionary mapping clustering method names to their labels.
    """
    plt.figure(figsize=(15, 12))
    titles = {
        'cluster_gmm': 'GMM Clustering Results',
        'cluster_kmeans': 'K-Means Clustering Results',
        'cluster_agglo': 'Agglomerative Clustering Results',
        'cluster_spectral': 'Spectral Clustering Results'
    }
    
    for i, (key, title) in enumerate(titles.items(), start=1):
        plt.subplot(2, 2, i)
        plt.scatter(X_encoded[:, 0], X_encoded[:, 1], c=cluster_labels[key], cmap='viridis', s=2)
        plt.title(title)
        plt.xlabel('Encoded Feature 1')
        plt.ylabel('Encoded Feature 2')
        plt.colorbar()
    
    plt.tight_layout()
    plt.show()


def main():
    # Parse command-line arguments for the data file path
    parser = argparse.ArgumentParser(description="VAE and Clustering Analysis")
    parser.add_argument('--data_path', type=str, default='path/to/your/datafile.txt',
                        help='Path to the input data file')
    args = parser.parse_args()
    
    # Load and preprocess the data
    X_df, X_scaled = load_and_preprocess_data(args.data_path)
    input_dim = X_scaled.shape[1]
    
    # Build and train the Variational Autoencoder
    vae, encoder = build_vae(input_dim=input_dim, latent_dim=2, intermediate_dim=128)
    train_vae(vae, X_scaled, epochs=100, batch_size=256)
    
    # Encode data to the latent space
    X_encoded = encoder.predict(X_scaled)
    
    # Perform clustering on the encoded data
    X_df = perform_clustering(X_encoded, X_df, n_clusters=10)
    
    # Compute and print cluster statistics for selected methods
    numeric_cols = ['error_proj', 'ac', 'cant_fotos', 'dist', 'med_ang']
    for method in ['cluster_gmm', 'cluster_kmeans', 'cluster_agglo', 'cluster_spectral']:
        stats = X_df.groupby(method)[numeric_cols].agg(['mean', 'std', 'size'])
        print(f"\n{method.upper()} Cluster Statistics:")
        print(stats)
    
    # Prepare cluster labels for evaluation and visualization
    cluster_labels = {
        'GMM': X_df['cluster_gmm'],
        'KMeans': X_df['cluster_kmeans'],
        'Agglomerative': X_df['cluster_agglo'],
        'Spectral': X_df['cluster_spectral'],
    }
    
    # Evaluate clustering performance
    evaluate_clusters(X_encoded, cluster_labels)
    
    # Visualize clustering results
    plot_clusters(X_encoded, {
        'cluster_gmm': X_df['cluster_gmm'],
        'cluster_kmeans': X_df['cluster_kmeans'],
        'cluster_agglo': X_df['cluster_agglo'],
        'cluster_spectral': X_df['cluster_spectral'],
    })


if __name__ == '__main__':
    main()
